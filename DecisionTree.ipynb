{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f779bf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#library imports \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d40e4a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data       \n",
    "y = data.target    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "823fa74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stratified data splitting\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.30,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=0.50,\n",
    "    stratify=y_temp,\n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9e0ee47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self,feature=None,threshold=None,left=None,right=None,*,value=None):\n",
    "        self.feature=feature\n",
    "        self.threshold=threshold\n",
    "        self.left=left\n",
    "        self.right=right\n",
    "        self.value=None\n",
    "        \n",
    "        \n",
    "    def is_leaf_node(self):\n",
    "        return self.value is not None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7df80dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, min_samples_split=2, max_depth=100, n_features=None):\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.n_features = n_features\n",
    "        self.root = None\n",
    "        self.feature_importances = {} # Added this to store scores\n",
    "\n",
    "    def fit(self,X,Y):\n",
    "        self.n_features=X.shape[1]\n",
    "        self.root = self.grow_tree(X, Y)\n",
    "\n",
    "    def grow_tree(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape\n",
    "        n_labels = len(np.unique(y))\n",
    "\n",
    "        if depth >= self.max_depth or n_labels == 1 or n_samples < self.min_samples_split:\n",
    "            leaf_value = np.bincount(y).argmax()\n",
    "            return Node(value=leaf_value)\n",
    "        else:\n",
    "            feat_idxs = np.random.choice(n_features, self.n_features, replace=False)\n",
    "            best_feat, best_thresh = self.best_split(X, y, feat_idxs)\n",
    "            left_idxs, right_idxs = self._split(X[:, best_feat], best_thresh)\n",
    "            left_child = self.grow_tree(X[left_idxs, :], y[left_idxs], depth + 1)\n",
    "            right_child = self.grow_tree(X[right_idxs, :], y[right_idxs], depth + 1)\n",
    "\n",
    "        return Node(feature=best_feat, threshold=best_thresh, left=left_child, right=right_child)\n",
    "    \n",
    "    def best_split(self,X,Y,feat_indcs):\n",
    "        best_IG=-1\n",
    "        split_feat,split_thr=None,None\n",
    "\n",
    "        for feat_idx in feat_indcs:\n",
    "            X_column = X[:, feat_idx]\n",
    "            unique_values = np.unique(X_column)\n",
    "            thresholds=[]\n",
    "\n",
    "            for i in range(len(unique_values) - 1):\n",
    "                midpoint = (unique_values[i] + unique_values[i+1]) / 2\n",
    "                thresholds.append(midpoint)\n",
    "\n",
    "            for thr in thresholds:\n",
    "                gain=self.in_gain(X_column,Y,thr)\n",
    "                if gain > best_IG:\n",
    "                    best_IG = gain\n",
    "                    split_feat = feat_idx\n",
    "                    split_thr = thr\n",
    "        \n",
    "        # Save the importance if a split was found\n",
    "        if split_feat is not None:\n",
    "            if split_feat not in self.feature_importances:\n",
    "                self.feature_importances[split_feat] = 0\n",
    "            self.feature_importances[split_feat] += best_IG\n",
    "\n",
    "        return split_feat,split_thr\n",
    "    \n",
    "    def entropy(self, y):\n",
    "        # 1. Count occurrences of each class label (e.g., [10, 40])\n",
    "        hist = np.bincount(y)\n",
    "        # 2. Convert counts to probabilities (e.g., [0.2, 0.8])\n",
    "        ps = hist / len(y)\n",
    "        # 3. Apply formula\n",
    "        #    CRITICAL: We filter (if p > 0) to avoid np.log2(0) which returns -inf\n",
    "        return -np.sum([p * np.log2(p) for p in ps if p > 0])\n",
    "\n",
    "    def _split(self,X_column,threshold):\n",
    "        left_idxs,right_idxs=[],[]\n",
    "        for i, val in enumerate(X_column):\n",
    "            if val<=threshold:\n",
    "                left_idxs.append(i)\n",
    "            else:\n",
    "                right_idxs.append(i)\n",
    "        return left_idxs,right_idxs\n",
    "\n",
    "\n",
    "    def in_gain(self,X_column,Y,threshold):\n",
    "        #1-Get H(Y)\n",
    "        target_counts= np.bincount(Y)\n",
    "        parent_entropy=self.entropy(Y)\n",
    "        #2-split the data\n",
    "        left_idxs, right_idxs = self._split(X_column, threshold)\n",
    "        if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
    "            return 0\n",
    "        #3-get H(Y|X)\n",
    "        entropy_left = self.entropy(Y[left_idxs])\n",
    "        entropy_right = self.entropy(Y[right_idxs])\n",
    "        child_entropy=len(left_idxs)/len(Y)*entropy_left+len(right_idxs)/len(Y)*entropy_right\n",
    "        #4-get Information gain\n",
    "        return parent_entropy-child_entropy\n",
    "    \n",
    "    def test(self, X):\n",
    "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
    "\n",
    "    def _traverse_tree(self, x, node):\n",
    "        if node.is_leaf_node():\n",
    "            return node.value\n",
    "        \n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        return self._traverse_tree(x, node.right)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e299051c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter tuning\n",
    "def tuning(X_train,Y_train,X_val,Y_val):\n",
    "    max_depths=[2,4,6,8,10]\n",
    "    min_samples_split=[2,5,10]\n",
    "    highest_acc=0\n",
    "    best_params={}\n",
    "    results_history = [] #useful when plotting\n",
    "\n",
    "    print(f\"{'Depth':<10} {'Min Split':<10} {'Val Accuracy':<15}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    for d in max_depths:\n",
    "        for s in min_samples_split:\n",
    "            #initialize tree\n",
    "            model=DecisionTree(max_depth=d, min_samples_split=s)\n",
    "            #train model using training dataset\n",
    "            model.fit(X_train,Y_train)\n",
    "            #test model using validation and training datasets\n",
    "            val_test=model.test(X_val)\n",
    "            train_test = model.test(X_train)\n",
    "            #calculate accuracy for both\n",
    "            val_acc = np.sum(val_test == y_val) / len(y_val)\n",
    "            train_acc = np.sum(train_test == y_train) / len(y_train)\n",
    "            #storing results for plotting later \n",
    "            results_history.append({\n",
    "            'max_depth': d,\n",
    "            'min_samples_split': s,\n",
    "            'validation accuracy': val_acc,\n",
    "            'train_accuracy': train_acc\n",
    "            })\n",
    "\n",
    "            print(f\"{d:<10} {s:<10} {val_acc:.4f}\")\n",
    "\n",
    "            #update the best acc so far\n",
    "\n",
    "            if val_acc > highest_acc:\n",
    "                highest_acc = val_acc\n",
    "                best_params = {'max_depth': d, 'min_samples_split': s}\n",
    "\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Winner: {best_params} with Acc: {highest_acc:.4f}\")\n",
    "\n",
    "    return best_params,results_history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a3ee5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth      Min Split  Val Accuracy   \n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<=' not supported between instances of 'float' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#retraining on training+validation\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m best_parameters, history = \u001b[43mtuning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mRetraining with best parameters...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m final_model = DecisionTree(\n\u001b[32m     10\u001b[39m max_depth=best_parameters[\u001b[33m'\u001b[39m\u001b[33mmax_depth\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     11\u001b[39m min_samples_split=best_parameters[\u001b[33m'\u001b[39m\u001b[33mmin_samples_split\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     12\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mtuning\u001b[39m\u001b[34m(X_train, Y_train, X_val, Y_val)\u001b[39m\n\u001b[32m     17\u001b[39m model.fit(X_train,Y_train)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m#test model using validation and training datasets\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m val_test=\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m train_test = model.test(X_train)\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m#calculate accuracy for both\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 92\u001b[39m, in \u001b[36mDecisionTree.test\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtest\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.array([\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_traverse_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m X])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 99\u001b[39m, in \u001b[36mDecisionTree._traverse_tree\u001b[39m\u001b[34m(self, x, node)\u001b[39m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m node.value\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x[node.feature] <= node.threshold:\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_traverse_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m.\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._traverse_tree(x, node.right)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 99\u001b[39m, in \u001b[36mDecisionTree._traverse_tree\u001b[39m\u001b[34m(self, x, node)\u001b[39m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m node.value\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x[node.feature] <= node.threshold:\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_traverse_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m.\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._traverse_tree(x, node.right)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 98\u001b[39m, in \u001b[36mDecisionTree._traverse_tree\u001b[39m\u001b[34m(self, x, node)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m node.is_leaf_node():\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m node.value\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m<\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthreshold\u001b[49m:\n\u001b[32m     99\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._traverse_tree(x, node.left)\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._traverse_tree(x, node.right)\n",
      "\u001b[31mTypeError\u001b[39m: '<=' not supported between instances of 'float' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "#retraining on training+validation\n",
    "best_parameters, history = tuning(\n",
    "X_train, y_train, \n",
    "X_val, y_val, \n",
    ")\n",
    "\n",
    "print(\"\\nRetraining with best parameters...\")\n",
    "\n",
    "final_model = DecisionTree(\n",
    "max_depth=best_parameters['max_depth'],\n",
    "min_samples_split=best_parameters['min_samples_split']\n",
    ")\n",
    "\n",
    "# Combine Train + Val\n",
    "X_combined = np.concatenate((X_train, X_val), axis=0)\n",
    "Y_combined = np.concatenate((y_train, y_val), axis=0)\n",
    "\n",
    "final_model.fit(X_combined, Y_combined)\n",
    "\n",
    "# Final Test\n",
    "test_predictions = final_model.test(X_test)\n",
    "final_acc = np.sum(test_predictions == y_test) / len(y_test)\n",
    "print(f\"Final Test Accuracy: {final_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d0007f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating metrics\n",
    "def calculate_metrics(y_true, y_pred, positive_class=1):\n",
    "    # True Positives: Predicted Positive AND Actually Positive\n",
    "    tp = np.sum((y_pred == positive_class) & (y_true == positive_class))\n",
    "    \n",
    "    # True Negatives: Predicted Negative AND Actually Negative\n",
    "    tn = np.sum((y_pred != positive_class) & (y_true != positive_class))\n",
    "    \n",
    "    # False Positives: Predicted Positive BUT Actually Negative\n",
    "    fp = np.sum((y_pred == positive_class) & (y_true != positive_class))\n",
    "    \n",
    "    # False Negatives: Predicted Negative BUT Actually Positive\n",
    "    fn = np.sum((y_pred != positive_class) & (y_true == positive_class))\n",
    "    \n",
    "    # Calculate Metrics (add epsilon 1e-7 to avoid division by zero)\n",
    "    precision = tp / (tp + fp + 1e-7)\n",
    "    recall = tp / (tp + fn + 1e-7)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + 1e-7)\n",
    "    \n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b823d0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plottings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Extract data for plotting ---\n",
    "# We want to see how Depth affects accuracy, so let's pick one split size (e.g., 2)\n",
    "fixed_split = 2 \n",
    "depths = []\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "\n",
    "for result in history:\n",
    "    if result['min_samples_split'] == fixed_split:\n",
    "        depths.append(result['max_depth'])\n",
    "        train_accs.append(result['train_accuracy'])\n",
    "        val_accs.append(result['val_accuracy'])\n",
    "\n",
    "# --- Create the Plot ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(depths, train_accs, label='Training Accuracy', marker='o', linestyle='--')\n",
    "plt.plot(depths, val_accs, label='Validation Accuracy', marker='o', linewidth=2)\n",
    "\n",
    "plt.title(f'Overfitting Analysis: Accuracy vs Max Depth (Split={fixed_split})')\n",
    "plt.xlabel('Max Depth')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --- Feature Importance Analysis ---\n",
    "print(\"\\n--- Feature Importance Ranking ---\")\n",
    "\n",
    "# 1. Get the dictionary from your trained model\n",
    "importances = final_model.feature_importances\n",
    "\n",
    "# 2. Convert indices to names (using data.feature_names)\n",
    "feature_names = data.feature_names\n",
    "sorted_features = sorted(importances.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# 3. Print Top 10\n",
    "print(f\"{'Rank':<5} {'Feature Name':<30} {'Importance Score':<15}\")\n",
    "print(\"-\" * 55)\n",
    "for i, (feat_idx, score) in enumerate(sorted_features[:10]):\n",
    "    print(f\"{i+1:<5} {feature_names[feat_idx]:<30} {score:.4f}\")\n",
    "\n",
    "print(\"\\n--- Final Performance Metrics (Test Set) ---\")\n",
    "\n",
    "# A. Metrics for Class 0 (Malignant)\n",
    "prec0, rec0, f1_0 = calculate_metrics(y_test, y_test_pred, positive_class=0)\n",
    "print(f\"Class 0 (Malignant): Precision={prec0:.3f}, Recall={rec0:.3f}, F1={f1_0:.3f}\")\n",
    "\n",
    "# B. Metrics for Class 1 (Benign)\n",
    "prec1, rec1, f1_1 = calculate_metrics(y_test, y_test_pred, positive_class=1)\n",
    "print(f\"Class 1 (Benign)   : Precision={prec1:.3f}, Recall={rec1:.3f}, F1={f1_1:.3f}\")\n",
    "\n",
    "# C. Confusion Matrix Details\n",
    "tp = np.sum((y_test_pred == 0) & (y_test == 0)) # Malignant correctly identified\n",
    "tn = np.sum((y_test_pred == 1) & (y_test == 1)) # Benign correctly identified\n",
    "fp = np.sum((y_test_pred == 0) & (y_test == 1)) # Benign labeled as Malignant\n",
    "fn = np.sum((y_test_pred == 1) & (y_test == 0)) # Malignant labeled as Benign (Dangerous!)\n",
    "\n",
    "print(\"\\n--- Confusion Matrix Analysis (Malignant as Positive) ---\")\n",
    "print(f\"True Positives (Cancer detected): {tp}\")\n",
    "print(f\"False Negatives (Cancer missed):  {fn}  <-- Critical number!\")\n",
    "print(f\"False Positives (False alarm):    {fp}\")\n",
    "print(f\"True Negatives (Healthy cleared): {tn}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
